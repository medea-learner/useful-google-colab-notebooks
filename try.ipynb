{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e10c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, pickle\n",
    "from operator import itemgetter\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1cf9d3",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8c6961",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4ae80da9abf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# *****************************************************************************\n",
    "def save_obj(obj, name, path):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : object\n",
    "        the object that will be saved.\n",
    "    name : string\n",
    "        the name of the saved file (pkl file).\n",
    "    path : string\n",
    "        the full path where obj will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    \n",
    "# *****************************************************************************\n",
    "def load_obj(name, path):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : string\n",
    "        the named of the pkl file that will be loaded.\n",
    "    path : string\n",
    "        the full path of the intended file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    type of the object within the file\n",
    "        the object in the specified file.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# *****************************************************************************\n",
    "def construct_objects(objs, objects):\n",
    "    features = []\n",
    "    for obj in objects:\n",
    "        count = 0\n",
    "        for o in objs:\n",
    "            if obj == o:\n",
    "                count += 1\n",
    "        features.append(count)\n",
    "    return features\n",
    "\n",
    "# *****************************************************************************\n",
    "def scan_objects(category_examples):\n",
    "    objects = set([])\n",
    "    for category in category_examples:\n",
    "        for instances in category_examples[category]:\n",
    "            for obj in instances:\n",
    "                objects.add(obj)\n",
    "            \n",
    "    return list(objects)\n",
    "\n",
    "   \n",
    "# ***************************************************************************** \n",
    "def load_dataset(dataset_name, learning_rate = 80, source_path = \"/content/\"):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name : string\n",
    "        # names : LabelMe /|\\ MITIndoor /|\\  Sun397 /|\\ Sun900\n",
    "\n",
    "    source_path : string, optional\n",
    "        specify the full path where the pkl file \"labelme_ce.pkl\" exist. The default is None.\n",
    "    learning_rate : float, optional\n",
    "        the learning rate. The default is 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    learning_instances : list\n",
    "        a list of instances having the format [([o1,o2,...],c), ([o3,o4,...], c), ...].\n",
    "    test_instances : list\n",
    "        a list of instances having the format [([o1,o2,...],c), ([o3,o4,...], c), ...].\n",
    "\n",
    "    '''\n",
    "    category_examples = load_obj(dataset_name + \"_ce\", source_path)\n",
    "    objects = load_obj(dataset_name + \"_o\", source_path)\n",
    "    \n",
    "    I_l = []\n",
    "    O_l = []\n",
    "    I_t = []\n",
    "    O_t = []\n",
    "    \n",
    "    nb_category = 0\n",
    "    for category in category_examples:\n",
    "        count = 1\n",
    "        point_of_division = len(category_examples[category]) * learning_rate/100\n",
    "        for objs in category_examples[category]:\n",
    "            constructed_objects = construct_objects(objs, objects)\n",
    "                        \n",
    "            if count <= point_of_division :\n",
    "                \n",
    "                I_l.append(constructed_objects)\n",
    "                O_l.append(category)\n",
    "            else:\n",
    "                I_t.append(constructed_objects)                \n",
    "                O_t.append(category)\n",
    "        \n",
    "            count += 1\n",
    "            \n",
    "        nb_category += 1\n",
    "    \n",
    "    return (I_l, O_l, I_t, O_t)\n",
    "\n",
    "# *****************************************************************************\n",
    "def getAccuracyRate(classifier, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Get accuracy rate of a classifier on test data.\n",
    "\n",
    "    Parameters:\n",
    "        classifier (_Classifier): The classification model.\n",
    "        X_test (list of [list of int]): List of test instances. Each instance is a list of features.\n",
    "        y_test (list of int): List of the classes corresponding to each test's instance.\n",
    "\n",
    "    Returns:\n",
    "        accuracyRate (float): Accuracy rate of the \"classifier\".\n",
    "    \"\"\"\n",
    "\n",
    "    cptCorrectPredict = 0\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        outValue = classifier.predict([X_test[i]])[0] # Output of the classifier on current instance (\"int\" value)\n",
    "\n",
    "        if outValue == y_test[i]: # If \"output\" equals \"target\" in the correct test instance: increment \"correct predictions\"\n",
    "            cptCorrectPredict += 1\n",
    "\n",
    "    return cptCorrectPredict / len(X_test) # Accuracy rate\n",
    "\n",
    "# *****************************************************************************\n",
    "def learning_algorithm(dataset_name, learning_rate = 80, nb_max_of_objects_to_keep1 = 10, nb_max_of_objects_to_keep2 = 10, source_path = \"/content/\"):\n",
    "    I_l, O_l, I_t, O_t  = load_dataset(dataset_name, learning_rate = learning_rate, source_path = source_path)\n",
    "    \n",
    "     # prepare prerequisites\n",
    "    print(\"prepare prerequisites :\")\n",
    "    prepare_prerequisites(dataset_name, learning_rate = learning_rate, source_path = source_path)\n",
    "    \n",
    "    # \n",
    "    objects = load_obj(dataset_name + \"_o\", source_path)\n",
    "    NO = load_obj(\"NO_\" + dataset_name, source_path)\n",
    "    FO = load_obj(\"FO_\" + dataset_name, source_path)\n",
    "    W = load_obj(\"W_\" + dataset_name, source_path)\n",
    "    \n",
    "    I_l1 = []\n",
    "    I_l2 = []\n",
    "    for l in range(0, len(I_l)):\n",
    "        index_value1 = []\n",
    "        index_value2 = []\n",
    "        test1 = []\n",
    "        test2 = []\n",
    "        for i in range(0, len(I_l[l])):\n",
    "            try:\n",
    "                value1 = W[objects[i]] * I_l[l][i]\n",
    "            except:\n",
    "                value1 = 0\n",
    "\n",
    "            try:\n",
    "                value2 = W[objects[i]] * I_l[l][i]/(1 + FO[O_l[l]][objects[i]] * math.log(NO[O_l[l]][objects[i]] + 2, 2))\n",
    "            except:\n",
    "                value2 = 0\n",
    "\n",
    "            test1.append(value1)\n",
    "            test2.append(value2)\n",
    "\n",
    "            if value1 != 0:                \n",
    "                index_value1.append((i, value1))\n",
    "                index_value2.append((i, value2))\n",
    "\n",
    "\n",
    "        index_value1 = sorted(index_value1, key=itemgetter(1), reverse=True)[:nb_max_of_objects_to_keep1]\n",
    "        index_value2 = sorted(index_value2, key=itemgetter(1), reverse=True)[:nb_max_of_objects_to_keep2]\n",
    "        for i in range(0, len(index_value1)):\n",
    "            test1[index_value1[i][0]] = index_value1[i][1]\n",
    "        for i in range(0, len(index_value2)):\n",
    "            test2[index_value2[i][0]] = index_value2[i][1]\n",
    "        \n",
    "        I_l1.append(test1)\n",
    "        I_l2.append(test2)\n",
    "    \n",
    "    classifiers = []\n",
    "    print(\"Classifier 1 :\") # ------ SVM ------\n",
    "    \n",
    "    # best_classifier = None\n",
    "    # best_accuracy = 0\n",
    "    # for kernel in [\"linear\"]: # 200, 300\n",
    "    #     for C in [0.5, 0.8, 1]:\n",
    "    #         for gamma in [\"scale\"]:\n",
    "    #             classifier = SVC(C = C, kernel = kernel, gamma=gamma).fit(I_l, O_l)\n",
    "    #             accuracy = getAccuracyRate(classifier, I_t, O_t)\n",
    "    #             if accuracy > best_accuracy:\n",
    "    #                 best_accuracy = accuracy\n",
    "    #                 best_classifier = classifier\n",
    "    # for kernel in [\"rbf\"]: # 200, 300\n",
    "    #     for C in [200, 300, 400]:\n",
    "    #         for gamma in [\"auto\"]:\n",
    "    #             classifier = SVC(C = C, kernel = kernel, gamma=gamma).fit(I_l, O_l)\n",
    "    #             accuracy = getAccuracyRate(classifier, I_t, O_t)\n",
    "    #             if accuracy > best_accuracy:\n",
    "    #                 best_accuracy = accuracy\n",
    "    #                 best_classifier = classifier\n",
    "    \n",
    "    best_classifier = SVC(C = 0.5, kernel = \"linear\", gamma=\"scale\").fit(I_l, O_l)\n",
    "    best_accuracy = getAccuracyRate(best_classifier, I_t, O_t)\n",
    "\n",
    "    print(best_accuracy)\n",
    "    classifiers.append(best_classifier)\n",
    "    save_obj([best_classifier, best_accuracy], \"classifier1_\" + dataset_name + \"_\" + str(learning_rate), source_path)\n",
    "\n",
    "    print(\"Classifier 2 :\") # ------ SVM ------\n",
    "\n",
    "    # best_classifier = None\n",
    "    # best_accuracy = 0\n",
    "    # for kernel in [\"linear\"]: # 200, 300\n",
    "    #     for C in [0.5, 0.8, 1]:\n",
    "    #         for gamma in [\"scale\"]:\n",
    "    #             classifier = SVC(C = C, kernel = kernel, gamma=gamma).fit(I_l1, O_l)\n",
    "    #             accuracy = getAccuracyRate(classifier, I_l1, O_l)\n",
    "    #             if accuracy > best_accuracy:\n",
    "    #                 best_accuracy = accuracy\n",
    "    #                 best_classifier = classifier\n",
    "    # for kernel in [\"rbf\"]: # 200, 300\n",
    "    #     for C in [200, 300, 400]:\n",
    "    #         for gamma in [\"auto\"]:\n",
    "    #             classifier = SVC(C = C, kernel = kernel, gamma=gamma).fit(I_l1, O_l)\n",
    "    #             accuracy = getAccuracyRate(classifier, I_l1, O_l)\n",
    "    #             if accuracy > best_accuracy:\n",
    "    #                 best_accuracy = accuracy\n",
    "    #                 best_classifier = classifier\n",
    "    \n",
    "    # print(best_accuracy)\n",
    "    # classifiers.append(best_classifier)\n",
    "\n",
    "\n",
    "    best_classifier = SVC(C = 0.5, kernel = \"linear\", gamma=\"scale\").fit(I_l1, O_l)\n",
    "    best_accuracy = getAccuracyRate(best_classifier, I_l1, O_l)\n",
    "\n",
    "    print(best_accuracy)\n",
    "    classifiers.append(best_classifier)\n",
    "    save_obj([best_classifier, best_accuracy], \"classifier2_\" + dataset_name + \"_\" + str(nb_max_of_objects_to_keep1) + \"_\" + str(learning_rate), source_path)\n",
    "\n",
    "    print(\"Classifier 3 :\") # ------ SVM ------\n",
    "    \n",
    "    # best_classifier = None\n",
    "    # best_accuracy = 0\n",
    "    # for kernel in [\"linear\"]: # 200, 300\n",
    "    #     for C in [0.5, 0.8, 1]:\n",
    "    #         for gamma in [\"scale\"]:\n",
    "    #             classifier = SVC(C = C, kernel = kernel, gamma=gamma).fit(I_l2, O_l)\n",
    "    #             accuracy = getAccuracyRate(classifier, I_l2, O_l)\n",
    "    #             if accuracy > best_accuracy:\n",
    "    #                 best_accuracy = accuracy\n",
    "    #                 best_classifier = classifier\n",
    "    # for kernel in [\"rbf\"]: # 200, 300\n",
    "    #     for C in [200, 300, 400]:\n",
    "    #         for gamma in [\"auto\"]:\n",
    "    #             classifier = SVC(C = C, kernel = kernel, gamma=gamma).fit(I_l2, O_l)\n",
    "    #             accuracy = getAccuracyRate(classifier, I_l2, O_l)\n",
    "    #             if accuracy > best_accuracy:\n",
    "    #                 best_accuracy = accuracy\n",
    "    #                 best_classifier = classifier\n",
    "    \n",
    "    # print(best_accuracy)\n",
    "    # classifiers.append(best_classifier)\n",
    "\n",
    "\n",
    "    best_classifier = SVC(C = 0.5, kernel = \"linear\", gamma=\"scale\").fit(I_l2, O_l)\n",
    "    best_accuracy = getAccuracyRate(best_classifier, I_l2, O_l)\n",
    "\n",
    "    print(best_accuracy)\n",
    "    classifiers.append(best_classifier)\n",
    "    save_obj([best_classifier, best_accuracy], \"classifier3_\" + dataset_name + \"_\" + str(nb_max_of_objects_to_keep2) + \"_\" + str(learning_rate), source_path)\n",
    "    \n",
    "    return classifiers\n",
    "\n",
    "# *****************************************************************************\n",
    "def testing_algorithm(dataset_name, classifiers, learning_rate = 80, nb_max_of_objects_to_keep1 = 10, nb_max_of_objects_to_keep2 = 10, source_path = \"/content/\"):\n",
    "    \n",
    "    _, _, I_t, O_t  = load_dataset(dataset_name, learning_rate = learning_rate, source_path = source_path)\n",
    "    \n",
    "    # \n",
    "    objects = load_obj(dataset_name + \"_o\", source_path)\n",
    "    NO = load_obj(\"NO_\" + dataset_name, source_path)\n",
    "    FO = load_obj(\"FO_\" + dataset_name, source_path)\n",
    "    dataset_categories = load_obj(\"categories_\" + dataset_name, source_path)\n",
    "    W = load_obj(\"W_\" + dataset_name, source_path)\n",
    "    \n",
    "    cptCorrectPredict = 0\n",
    "    \n",
    "    for l in range(0, len(I_t)):\n",
    "        \n",
    "        categories = []\n",
    "        \n",
    "        # classifier 1\n",
    "        outValue_without_normalization1 = classifiers[0].predict([I_t[l]])[0]\n",
    "\n",
    "        # classifier 2\n",
    "        for category in dataset_categories:\n",
    "            index_value = []\n",
    "            test = []\n",
    "            not_null_objects = 0\n",
    "            for i in range(0, len(I_t[l])):\n",
    "                try:\n",
    "                    value = W[objects[i]] * I_t[l][i]\n",
    "                except:\n",
    "                    value = 0\n",
    "                \n",
    "                test.append(value)\n",
    "\n",
    "                if value != 0:\n",
    "                    index_value.append((i, value))\n",
    "\n",
    "            \n",
    "            index_value = sorted(index_value, key=itemgetter(1), reverse=True)[:nb_max_of_objects_to_keep1]\n",
    "            \n",
    "            for i in range(0, len(index_value)):\n",
    "                test[index_value[i][0]] = index_value[i][1]\n",
    "            \n",
    "            outValue_with_normalization1 = classifiers[1].predict([test])[0]\n",
    "\n",
    "            if outValue_with_normalization1 == category:\n",
    "                categories.append(category)\n",
    "        \n",
    "        if len(categories) == 1:\n",
    "            if categories[0] == O_t[l]:\n",
    "                cptCorrectPredict += 1\n",
    "                continue\n",
    "        categories = []\n",
    "        \n",
    "        # classifier 3\n",
    "        for category in dataset_categories:\n",
    "            index_value = []\n",
    "            test = []\n",
    "            not_null_objects = 0\n",
    "            for i in range(0, len(I_t[l])):\n",
    "                try:\n",
    "                    value = W[objects[i]] * I_t[l][i]/(1 + FO[category][objects[i]] * math.log(NO[category][objects[i]] + 2, 2))\n",
    "                except:\n",
    "                    value = 0\n",
    "                \n",
    "                test.append(value)\n",
    "\n",
    "                if value != 0:\n",
    "                    index_value.append((i, value))\n",
    "\n",
    "            \n",
    "            index_value = sorted(index_value, key=itemgetter(1), reverse=True)[:nb_max_of_objects_to_keep2]\n",
    "            \n",
    "            for i in range(0, len(index_value)):\n",
    "                test[index_value[i][0]] = index_value[i][1]\n",
    "            \n",
    "            \n",
    "            outValue_with_normalization2 = classifiers[2].predict([test])[0]\n",
    "            \n",
    "            \n",
    "            \n",
    "            if outValue_with_normalization2 == category:\n",
    "                categories.append(category)\n",
    "                                                                               \n",
    "        \n",
    "        print(l, \" => \", not_null_objects)        \n",
    "        \n",
    "        if len(categories) == 1:\n",
    "            if categories[0] == O_t[l]:\n",
    "                cptCorrectPredict += 1\n",
    "                continue\n",
    "        \n",
    "        \n",
    "\n",
    "        if outValue_without_normalization1 == O_t[l]:\n",
    "            cptCorrectPredict += 1\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "    print(cptCorrectPredict / len(I_t)) # Accuracy rate\n",
    "    \n",
    "\n",
    "\n",
    "# *****************************************************************************\n",
    "def prepare_prerequisites(dataset_name, learning_rate = 80, source_path = \"/content/\"):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name : string\n",
    "        DESCRIPTION.\n",
    "    Max_value : TYPE\n",
    "        DESCRIPTION.\n",
    "    source_path : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    # **************************************************\n",
    "    objects = load_obj(dataset_name + \"_o\", source_path)\n",
    "    I_l, O_l, _, _  = load_dataset(dataset_name, learning_rate = learning_rate, source_path = source_path)\n",
    "    \n",
    "    # **************************************************\n",
    "    category_examples = {}\n",
    "    for i in range(0, len(O_l)):\n",
    "        try:\n",
    "            category_examples[O_l[i]].append(I_l[i])\n",
    "        except:\n",
    "            category_examples[O_l[i]] = [I_l[i]]\n",
    "        \n",
    "    # **************************************************\n",
    "    # generate two dictionaries emission_probabilities = { (SCi,Oi):value, ..}\n",
    "    # and weights = { Oi:value }\n",
    "    NO = {}\n",
    "    FO = {}\n",
    "    \n",
    "    for category in category_examples:\n",
    "        NO[category] = {}\n",
    "        FO[category] = {}\n",
    "        \n",
    "        for instance in category_examples[category]:\n",
    "            for i in range(0, len(instance)):\n",
    "                if instance[i] > 0:\n",
    "                    try:\n",
    "                        NO[category][objects[i]] = NO[category][objects[i]] + 1\n",
    "                        FO[category][objects[i]] = FO[category][objects[i]] + instance[i]\n",
    "\n",
    "                    except:\n",
    "                        NO[category][objects[i]] = 1\n",
    "                        FO[category][objects[i]] = instance[i]\n",
    "                        \n",
    "    # **************************************************\n",
    "    NOall = {}\n",
    "    FOall = {}\n",
    "    \n",
    "    for obj in objects:\n",
    "        NOall[obj] = 0\n",
    "        FOall[obj] = 0\n",
    "    \n",
    "    for category in NO:\n",
    "        for obj in NO[category]:\n",
    "            NOall[obj] = NOall[obj] + NO[category][obj]\n",
    "            FOall[obj] = FOall[obj] + FO[category][obj]\n",
    "                            \n",
    "    # **************************************************\n",
    "    emission_probabilities = {}\n",
    "    \n",
    "    for category in NO:\n",
    "        emission_probabilities[category] = {}\n",
    "        for obj in NO[category]:\n",
    "            emission_probabilities[category][obj] = FO[category][obj] * math.log(NO[category][obj] + 1, 10) - math.log(NOall[obj] * FOall[obj], 10)\n",
    "    \n",
    "    # **************************************************\n",
    "    W = {}\n",
    "    \n",
    "    for obj in objects:\n",
    "        W[obj] = 0\n",
    "        if NOall[obj] != 0:\n",
    "            # -----------------------------\n",
    "            max_NO = 0\n",
    "            max_FO = 0\n",
    "            for category in NO:\n",
    "                try:\n",
    "                    if NO[category][obj] > max_NO:\n",
    "                        max_NO = NO[category][obj]\n",
    "                    if FO[category][obj] > max_FO:\n",
    "                        max_FO = FO[category][obj]\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            W[obj] = max_FO * math.log(max_NO + 1, 10) - math.log((NOall[obj] + 1) * FOall[obj], 10)\n",
    "    \n",
    "\n",
    "    # **************************************************\n",
    "    save_obj(NO, \"NO_\" + dataset_name, source_path)\n",
    "    save_obj(FO, \"FO_\" + dataset_name, source_path)\n",
    "    save_obj(list(category_examples.keys()), \"categories_\" + dataset_name, source_path)\n",
    "    save_obj(W, \"W_\" + dataset_name, source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0bf4bb",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************\n",
    "# names : LabelMe /|\\ MITIndoor /|\\ Sun397 /|\\ Sun900\n",
    "source_path = \"/content/\"\n",
    "dataset_name = \"Sun397\"\n",
    "\n",
    "# params\n",
    "learning_rate = 80\n",
    "nb_max_of_objects_to_keep1 = 10\n",
    "nb_max_of_objects_to_keep2 = 10\n",
    "\n",
    "\n",
    "print(\"learn :\")\n",
    "# classifiers = load_obj(dataset_name + \"_classifiers_\" + str(learning_rate) + \"%\", source_path)\n",
    "classifiers = learning_algorithm(dataset_name, learning_rate = learning_rate, nb_max_of_objects_to_keep1 = nb_max_of_objects_to_keep1, nb_max_of_objects_to_keep2 = nb_max_of_objects_to_keep2, source_path = source_path)\n",
    "save_obj(classifiers, dataset_name + \"_classifiers_\" + str(learning_rate) + \"%\", source_path)\n",
    "print(\"test :\")\n",
    "testing_algorithm(dataset_name, classifiers, learning_rate = learning_rate, nb_max_of_objects_to_keep1 = nb_max_of_objects_to_keep1, nb_max_of_objects_to_keep2 = nb_max_of_objects_to_keep2, source_path = source_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
